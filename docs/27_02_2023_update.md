# Weather Translation in Images - 27.03.2023 Update

This is an update on the results obtained after playing around with the idea of image translation and a naive model and small amount of data.

## **Weather2Weather** dataset
The Weather2Weather dataset is supposed to be a dataset containing image data from webcams over the world, along with some synthetic metadata regarding weather conditions. The purpose of this is to easily pair images from the same location and different weather categories.

General algorithm for creating the dataset.
1. Download a lot of 1-year streams using the [Windy API](windy.com) . Each stream should contain between 10 and 50 images evenly distributed over a calendar year, and hopefully a wide variety of weather condition.
2. For each stream downloaded, take each image in it and recognize the weather conditions present and (optional) weather cues. Store results in stream associated JSON file.
3. Validate each stream


A minimal version of the dataset I've created can be found [here](https://drive.google.com/file/d/1ub8PguYJ2njc0Nk26yKeBj_bse7nKOY_/view?usp=share_link). It contains around 93 streams (sampled from 700 before validation), summing to 4023 images (25k before validation), somewhat evenly distributed between cloudy and clear weather (overall, not necessarily per indivudal stream).

Based on my experience, I expect a proper stream to contain at least around 1000 streams and 40k images, but it will take a lot of work. I need to define the end goal as clear as possible and make sure that the inference model is as good as possible for its purpose.

---

### 1. Stream Scraping
This part is done by scraping streams from the [Windy API](www.windy.com). While the API is usage-restricted, it can fit our task well-enough: scraped around 700 streams with 25k images with no issues. A quick implementation of the scraper can be found at [code/dataset/dataset_scraper.py](https://github.com/thesstefan/vae_weather_translation/blob/7dd5cdb24a9019010cd9cb44a50212819212dbb8/code/dataset/dataset_scraper.py).

### 2. Weather Classification and (possibly) Weather-Cue Segmentation
For each image in the dataset, we want to provide some class labels denoting the weather categories present in the image, as well as a segmentation map with some helpful cues (denoting ground, clear sky, clouds etc.).

#### Naive Weather Recognition Attempt

Played around for two days with own ResNet50 implementations for weather recognition on [Image2Weather](https://sci-hub.se/https://ieeexplore.ieee.org/document/7545010) and the [Multi-class Weather Image Dataset](https://mwidataset.weebly.com/). While results were decent on training and validation data, they were very bad (practically random) on test images - some results can be found in this [document](https://docs.google.com/document/d/1BlW9mkE2I817IRug91YIMiqbX2w-PXKHnJJtG9vAiBc/edit?usp=sharing) and a notebook example can be seen [here](https://github.com/thesstefan/vae_weather_translation/tree/1f027c9214f92e83b7fd2f0ab718b2b97291354d/notebooks).

> Even if initial results were not ideal, it might be worth experimenting more with models (especially already existing ones like the one in [Weather Classification with Deep Convolutional Neural Networks](https://sci-hub.se/https://ieeexplore.ieee.org/document/7351424)) that only classify weather conditions. Since we only need *decent* accuracy, simplifying this part and playing more with the translation model itself could be worth it.

#### Multi-Task Weather Recognition and Segmentation Model

Specifically, we're talking about the multi-task model proposed in [*Weather Recognition via Classification Labels and Weather-Cue Maps*](https://sci-hub.se/https://www.sciencedirect.com/science/article/pii/S0031320319302481). While using this seems ideal, it has a many issues:

 - **Critical**: The authors only provide model code, along with a pretrained model on Clear/Cloudy weather recognition and segmentation. The paper also mentions good results for  snowy, rainy and foggy weather. Ideally, we'd also be able to work with those, but since we have no dataset, we'd have to create it ourselves. This kind of dataset involves a lot of manual work: each images needs bounding boxes for weather cues and weather category labels. 
 	- **Solution**: Create our own dataset for this task using a subset of other available traditional weather datasets (without weather-cue segmentation) like [Image2Weather](https://sci-hub.se/https://ieeexplore.ieee.org/document/7545010) or the [Multi-class Weather Image Dataset](https://mwidataset.weebly.com/) and add bounding boxes for weather cues.
- The implementation provided by the authors is written in Python2.7 and TensorFlow ([here](https://github.com/wzgwzg/Multitask_Weather)), which is far from ideal, since it doesn't fit well with the rest of the codebase/tools (somewhat hard to run on things like colab and also integrate with modern python code). For now, a simplified version for inference along with a compatible Python `env` and Docker configuration is provided in [code/weather_classifier](https://github.com/thesstefan/vae_weather_translation/tree/1f027c9214f92e83b7fd2f0ab718b2b97291354d/code/weather_classifier). 
	- **Solution (Not a priority)**: Write own modern implementation using Python3.10+ and `pytorch`.
		- Initially, it may be useful to try converting the pretrained TF1 model to a torch model using the [ONNX converter](https://github.com/onnx/tensorflow-onnx).

### 3. Stream Validation
After the scraping and weather inference is done, we want to see if the quality of data is good enough.

For each image in each stream we check if:
- Stream images are useful:
	+ There are no defects/artifacts
	+ There are no unusuable images e.g. night-time or heavy snowstorm. Note that the stream images scraped are most of the time (I'd say 95%+) day-time images.
	+ Images are from the same POV in the same steram i.e. the camera is not moving and perspective is the same in all images from a stream
- Inferred weather conditions are accurate:
	+ Label is assigned correctly (preferrably with high probability)
	+ Weather segmentation (optional) has a good enough accuracy

We do this with a tool similar to the one shown below:

![Dataset Tool](https://github.com/thesstefan/vae_weather_translation/blob/main/paper/images/dataset_tool.png?raw=true "Dataset Tool")

## Weather Translation using VAEs

### VAE Theory
I feel like I've developed a decent high-level intuition on VAEs and their math from some articles and the original paper [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114). I'm trying to develop deeper insight of the math involved by going through [An Introduction to
Variational Autoencoders](https://arxiv.org/pdf/1906.02691.pdf).

### Playing around with VAEs
Unfortunately, I didn't have a lot of time for this (exam session + life) :(. I've tried two very naive approaches for Clear->Cloudy conversions, but lost some training results due to running out of time on Colab, Kaggle crashing when memory was full.

Generally, a VAE that translates weather from clear images to cloudy image is trained by trying to reconstruct a cloudy version of the image from the clear image. For this, we create `(CLEAR_IMAGE, CLOUD_IMAGE)` pairs as training/validation data points. We use the usual VAE training method, but try to reconstruct a different image.

#### Very Basic Convolutional VAE
[**Notebook**](https://www.kaggle.com/code/stefanstefanache/weather-basic-vae)

Used 300x300 images. After ~5 minutes the model converges. It seems to do relatively decent on the training dataset, but performs poorly on test data. The learned distribution is far from ideal, since we'd want more "lively". Hard to say more, but I generally feel like the encoder is the weak part here.

#### ResNet VAE using Pytorch Lightning
[**Notebook**](https://www.kaggle.com/code/stefanstefanache/weather-resnet-vae)

Uses a VAE architecture where the encoder is a ResNet18 model and the decoder is something equivalent to its inverse.

Ran on images resized to 96x96. Training tooks around 4 hours, but lost training results because of iteration on notebook and Kaggle crashing. :( The provided notebook shows results for pretrained model.

Don't have training statistics, but loss was declining steadily. However, the model seems to heavily overfit the data. The samples from the distribution are very similar to the images in the training set. The decoder could definitely do a better job in reconstructing the images. Some kind of blending with the source image for "non-weather" sections could be useful.

Overfitting is a problem because of the relatively small dataset. I suspect that increasing the dataset size by about... 5 times should diminuate it.

#### Some Takeaways
1. There is a *LOT* of experimentation left to be done and ways of improving the model, be it through the use of segmentation or through the model's architecture.
2. While the models seem to learn a *reasonable* idea of what making the sky in an image cloudy means, it's not exactly what someone is looking for. While making the whole sky gray and depressing is technically correct, ideally I'd also like to see some cloud details. This is a shortcoming of VAEs since they learn an average of what cloudy means and this is what the average actually is (especially according to our dataset where a lot of cloudy images only have gray sky.
3. The more powerful model is overfitting. The obvious solution to this is expanding the dataset with a *lot* more images. Currently, 93 streams is definitely not enough.
4. I feel like there are a lot of ways we could use the segmentation masks to enhance the performance of the model:
	1.  We could us some attention-like mechanism where we make the encoder focus on weather-cues in the image, and the decoder look at "what is not weather" in the source image and try to replicate it.
	2. Another way of using it is do some kind of postprocessing on the VAE output and blend the source/target image "non-weather" details with the source image, so that only actual weather is translated.

---

## Conclusions
### Goals
 The original goals of the project are:
1. Learning more about generative models, especially about autoencoders
2. Trying to get better performance than [WeatherGAN](https://arxiv.org/pdf/2103.05422.pdf) on the weather translation task through the usage of paired images from the same location and a VAE based model

While the first goal is definitely being achieved, I don't have a lot of hope into achieving the second, due to the large amount of data/time/power required. 

Therefore, I need to create a realistic plan for what I want to achieve for the thesis, since the original plan requires way more work than expected. Some ideas:
- Play around with some existing generic paired translation models unrelated to VAEs ([BicycleGAN](https://junyanz.github.io/CycleGAN/), [InstaFormer](https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_InstaFormer_Instance-Aware_Image-to-Image_Translation_With_Transformer_CVPR_2022_paper.pdf)) with some fine-tuning and try to get better performance than [WeatherGAN](https://arxiv.org/pdf/2103.05422.pdf), while leveraging the power of the **weather2weather** paired dataset. If it does not work, make some kind of "study" around it. Basically, the paper would showcase the capabilities of the newly created dataset.
- Work more on the VAE model and focus only on cloudy/clear translations, maybe adding some other parts to the model architecture, like an attention mechanism.

### Priorities
My goal is to learn as much as possible regarding ML and maybe do some work that has the potential of being published sometime in the near (6 months - 1 year) future.

Regarding the whole scope of the project, the presentation layer shouldn't be a problem. I plan to create a simple flask+react server running on cloud, simply showing how translations are done. If the model is done, this shouldn't take more than a couple of days.

The critical thing now is to define a clear view of what the end goal is and create a realistic plan to achieve it. 




